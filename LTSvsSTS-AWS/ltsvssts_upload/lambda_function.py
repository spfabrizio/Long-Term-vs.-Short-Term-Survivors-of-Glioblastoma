#
# Uploads a json file to S3 and then inserts a new job record
# in the LTSvsSTS database with a status of 'uploaded'.
# Sends the job id back to the client.
#

import json
import boto3
import os
import uuid
import base64
import pathlib
import datatier

from configparser import ConfigParser

def lambda_handler(event, context):
  try:
    print("**STARTING**")
    print("**lambda: ltsvssts_upload**")
    
    # setup AWS based on config file:
    config_file = 'ltsvsstsapp-config.ini'
    os.environ['AWS_SHARED_CREDENTIALS_FILE'] = config_file
    
    configur = ConfigParser()
    configur.read(config_file)
    
    # configure for S3 access:
    s3_profile = 's3readwrite'
    boto3.setup_default_session(profile_name=s3_profile)
    
    bucketname = configur.get('s3', 'bucket_name')
    
    s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucketname)
    
    # configure for RDS access
    rds_endpoint = configur.get('rds', 'endpoint')
    rds_portnum = int(configur.get('rds', 'port_number'))
    rds_username = configur.get('rds', 'user_name')
    rds_pwd = configur.get('rds', 'user_pwd')
    rds_dbname = configur.get('rds', 'db_name')
    
    # computeid from event: could be a parameter
    # or could be part of URL path ("pathParameters"):
    print("**Accessing event/pathParameters**")
    
    if "computeid" in event:
      computeid = event["computeid"]
    elif "pathParameters" in event:
      if "computeid" in event["pathParameters"]:
        computeid = event["pathParameters"]["computeid"]
      else:
        raise Exception("requires computeid parameter in pathParameters")
    else:
        raise Exception("requires computeid parameter in event")
        
    print("computeid:", computeid)
  
    #
    # the user has sent us two parameters:
    #  1. filename of their file
    #  2. json file of thresholds and phenotype configurations
    #
    # The parameters are coming through web server 
    # (or API Gateway) in the body of the request
    # in JSON format.
    #
    print("**Accessing request body**")
    
    if "body" not in event:
      raise Exception("event has no body")
      
    body = json.loads(event["body"]) # parse the json
    
    if "filename" not in body:
      raise Exception("event has a body but no filename")
    if "data" not in body:
      raise Exception("event has a body but no data")

    filename = body["filename"]
    template_json = body["data"]
    
    print("filename:", filename)
    print(template_json['THRESHOLDS'])

    # open connection to the database:
    print("**Opening connection**")
    dbConn = datatier.get_dbConn(rds_endpoint, rds_portnum, rds_username, rds_pwd, rds_dbname)
    
    # Prepare to store as JSON file locally:
    print("**Writing local JSON data file**")
    basename = pathlib.Path(filename).stem
    extension = pathlib.Path(filename).suffix

    if extension != ".json": 
      raise Exception("expecting filename to have .json extension")

    local_filename = "/tmp/data.json"

    with open(local_filename, "w") as outfile:
      json.dump(template_json, outfile)

    # generate unique filename in preparation for the S3 upload:
    print("**Uploading local file to S3**")

    bucketkey = "LTSvsSTS-Template/" + basename + "-" + str(uuid.uuid4()) + ".json"
    if int(computeid)==1:
      bucketkey = "LTSvsSTS1-Template/" + basename + "-" + str(uuid.uuid4()) + ".json"
    elif int(computeid)==2:
      bucketkey = "LTSvsSTS2-Template/" + basename + "-" + str(uuid.uuid4()) + ".json"
    elif int(computeid)==3:
      bucketkey = "LTSvsSTS3-Template/" + basename + "-" + str(uuid.uuid4()) + ".json"
    elif int(computeid)==4:
      bucketkey = "LTSvsSTS4-Template/" + basename + "-" + str(uuid.uuid4()) + ".json"
    else:
      raise Exception("invalid computeid")

    print("S3 bucketkey:", bucketkey)

    # Remember that the processing of the PDF is event-triggered,
    # and that lambda function is going to update the database as
    # is processes. Insert job record then upload JSON file.
    print("**Adding jobs row to database**")
    
    sql = """
      INSERT INTO jobs(computeid, status, originaldatafile, datafilekey, resultsfilekey)
                  VALUES(%s, %s, %s, %s, '');
    """
    
    status = 'uploaded'
    datatier.perform_action(dbConn, sql, [computeid, status, filename, bucketkey])

    # Grab the jobid that was auto-generated by mysql:
    sql = "SELECT LAST_INSERT_ID();"
    
    row = datatier.retrieve_one_row(dbConn, sql)
    
    jobid = row[0]
    
    print("jobid:", jobid)

    print("**Uploading data file to S3**")
    #bucket.upload_file(???, 
    #                   bucketkey, 
    #                   ExtraArgs={
    #                     'ACL': 'public-read',
    #                     'ContentType': 'application/pdf'
    #                   })

    bucket.upload_file(local_filename, 
                       bucketkey, 
                       ExtraArgs={
                         'ACL': 'public-read',
                         'ContentType': 'application/json'
                       })

    
    #
    # respond in an HTTP-like way, i.e. with a status
    # code and body in JSON format:
    #
    print("**DONE, returning jobid**")
    
    return {
      'statusCode': 200,
      'body': json.dumps(str(jobid))
    }
    
  except Exception as err:
    print("**ERROR**")
    print(str(err))
    
    return {
      'statusCode': 500,
      'body': json.dumps(str(err))
    }